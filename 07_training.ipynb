{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ML models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have explored the repository and selected an appropriate training dataset, you can stage it and train a model. In this notebook we will show an example of how to do so using the [EuroSAT](https://www.eotdl.com/datasets/EuroSAT-RGB) dataset, in its RGB version.\n",
    "\n",
    "> Remember that you can run this notebook in your cloud workspace to train a model in the cloud. If you require a GPU-powered machine, let us know though Discord and we will provide one!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Staging assets: 100%|██████████| 1/1 [00:02<00:00,  2.58s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/EuroSAT-RGB'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eotdl.datasets import stage_dataset\n",
    "\n",
    "path = stage_dataset(\"EuroSAT-RGB\", version=1, path=\"data\", force=True, assets=True)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace data/train/EuroSAT-RGB/Industrial/Industrial_1743.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip -q {path}/EuroSAT-RGB.zip -d data/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This might take a few minutes!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to streamline the training process, we will use the [PytorchEO](https://github.com/earthpulse/pytorchEO) library. This open source library is built on top of [Pytorch](https://pytorch.org/) and [Pytorch Lightning](https://lightning.ai/) to facilitate the design, implementation, training and deployment of deep learning models for Earth Observation. It offers AI-Ready EO datasets as well as ready-to-use tasks and models.\n",
    "\n",
    "We can use the EuroSATRGB wrapper to pre-process the dataset, including the generation of classes, train/val/test splitting, etc. We plan to implement more wrappers for other datasets in the future, but feel free to contribute with your own (either to PytorchEO or any other library).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_eo.datasets import EuroSATRGB\n",
    "\n",
    "# do not set download to True since it will download the dataset from the original source and not EOTDL !\n",
    "\n",
    "ds = EuroSATRGB(\n",
    "    batch_size=25,\n",
    "    verbose=True,\n",
    "    path=\"data\",\n",
    "    download=False,\n",
    "    data_folder=\"train/EuroSAT-RGB\",\n",
    ")\n",
    "\n",
    "ds.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 27000 Sentinel 2 images classified in 10 categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.num_classes, ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch = next(iter(ds.train_dataloader()))\n",
    "imgs, labels = batch[\"image\"], batch[\"label\"]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i, (img, label) in enumerate(zip(imgs, labels)):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    ax.set_title(ds.classes[label.item()])\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a neural network for the task of Image classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_eo.tasks.classification import ImageClassification\n",
    "\n",
    "task = ImageClassification(num_classes=ds.num_classes)\n",
    "\n",
    "sample_inputs = torch.randn(8, 3, 224, 224)\n",
    "output = task(sample_inputs)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    max_epochs=5,\n",
    "    enable_checkpointing=False,\n",
    "    limit_train_batches=50,\n",
    "    limit_val_batches=50,\n",
    ")\n",
    "\n",
    "trainer.fit(task, ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained you can evaluate it using some test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "task.eval()\n",
    "acc = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(ds.test_dataloader()):\n",
    "        output = task(batch[\"image\"])\n",
    "        acc += (output.argmax(1) == batch[\"label\"]).sum().item()\n",
    "\n",
    "print(\"test accuracy: \", f\"{acc}/{len(ds.test_dataloader().dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract batch from test dataloader\n",
    "\n",
    "batch = next(iter(ds.test_dataloader(shuffle=True, batch_size=25)))\n",
    "imgs, labels = batch[\"image\"], batch[\"label\"]\n",
    "\n",
    "# compute predictions\n",
    "\n",
    "preds = task.predict(batch)\n",
    "preds = torch.argmax(preds, axis=1)\n",
    "\n",
    "# visualize predictions\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i, (img, label, pred) in enumerate(zip(imgs, labels, preds)):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    gt = ds.classes[label.item()]\n",
    "    pred = ds.classes[pred.item()]\n",
    "    ax.set_title(gt, color=\"green\" if gt == pred else \"red\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And export it to later ingestion to the EOTDL. You can choose your preferred export method, here we use [ONNX]().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filepath = \"outputs/model.onnx\"\n",
    "os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "task.to_onnx(\n",
    "    filepath,\n",
    "    imgs,\n",
    "    export_params=True,\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(filepath)\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "ort_inputs = {input_name: imgs.numpy()}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "ort_outs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for batch in tqdm(ds.test_dataloader()):\n",
    "    ort_inputs = {input_name: batch[\"image\"].numpy()}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    acc += np.sum((ort_outs[0].argmax(1) == batch[\"label\"].numpy()))\n",
    "\n",
    "print(\"test accuracy: \", f\"{acc}/{len(ds.test_dataloader().dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will learn how to ingest this model to EOTDL in the nex notebook.\n",
    "\n",
    "This has been a fast and easy example on how to train an ML model with a datasets downloaded from EOTDL. It is not our goal to provide a complete tutorial on how to train a model, but rather to show how to use the EOTDL. If you want to learn more about AI and training deep neural networks, we encourage you to explore the [PytorchEO](https://github.com/earthpulse/pytorchEO) library, and even contribute with more datasets, tasks, models and wrappers. We challenge you to train a better model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and Contribution opportunities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to ask questions now (live or through Discord) and make suggestions for future improvements.\n",
    "\n",
    "- What would you like to see in the EOTDL concerning training?\n",
    "- What are the main challenges you face when training ML models with EO data?\n",
    "- What are the main datasets you would like to see in the EOTDL?\n",
    "- What are the main tasks you would like to see implemented?\n",
    "- What are the main models you would like to see implemented?\n",
    "- What are the frameworks you would like to have wrappers for?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
